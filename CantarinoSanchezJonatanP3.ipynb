{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomanació basada en PageRank\n",
    "\n",
    "L'algorisme PageRank, famós per l'ús en el cercador de Google, te moltes altres aplicacions. Un exemple d'ells és la recomanació d'items a usuaris basat en les similituds entre usuaris.\n",
    "\n",
    "Recordeu el principi de funcionament bàsic de les recomanacions colaboratives basades en l'usuari, obtenim una puntuació per a un item i usuari, basat en les similituds d'aquest usuari amb la resta, i respectives puntuacions. Si ho formulem, podríem dir que tot es basa en:\n",
    "\n",
    "$$\\hat{r}_{u,i} = \\frac{\\sum_{v,v\\neq u} sim(u,v) \\cdot r_{v,i}}{\\sum_{v,v\\neq u} sim(u,v)}$$\n",
    "\n",
    "Amb notació \n",
    "\n",
    "* $r_{u,i}$ ens indica la puntuació ($r$ating) de l'usuari $u$ a l'item $i$. \n",
    "* El sumatori $\\sum_{v,v\\neq u}$ indica la suma per cada usuari $v$ que no sigui el propi $u$ del que estem intentant predir una puntuació\n",
    "* El barret a $\\hat{r}$ denota que es tracta d'una predicció, el valor que estem intentant inferir a partir de les dades.\n",
    "\n",
    "La funció $sim$ és la que ens indica quan semblants són dos usuaris entre sí. Tal i com heu vist a teoria, això es pot fer amb mètriques com la distància euclidea o la similitud de Pearson, d'entre moltes altres. En aquesta pràctica, però, veure'm com el vector obtingut a partir de calcular PageRank sobre una matriu concreta d'usuaris i items, també ens proporciona una mesura de similitud significativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia\n",
    "\n",
    "Per aquells interesats, tota la informació relativa a PageRank està basada en la publicació\n",
    "\n",
    "`Bryan, K., & Leise, T. (2006). The $25,000,000,000 eigenvector: The linear algebra behind Google. Siam Review, 48(3), 569-581.`\n",
    "\n",
    "I els algorismes matemàtiques en el llibre:\n",
    "\n",
    "`Applied numerical linear algebra, James W. Demmel`, capítol 4.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank\n",
    "\n",
    "Aquesta cel·la serveix com a una breu recapitulació de l'algorisme PageRank vist a teoria. Recordem que PageRank es basa en trobar l'importància de les pàgines en base a la reputació d'aquestes i del número de links entrants i sortints.\n",
    "\n",
    "Supossa que tenim la següent estructura de pàgines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/page1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "És a dir, si comptem el número de `in-link` (enllaços d'entrada) de cada pàgina $x$, obtindríem:\n",
    "$$x_1=3,x_2=2,x_3=1,x_4=3$$\n",
    "\n",
    "Però també podem expressar-ho en funció de la pàgina de qui rebem el link, de forma que tinguem la importància en compte:\n",
    "\n",
    "$$x_{entrada} = \\dfrac{x_{sortida}}{|x_{sortida}|}$$\n",
    "\n",
    "Per exemple, si ho apliquem a $x_1$, $x_1 = x_2 / 2 + x_3 / 2 + x_4 / 3$. De forma semblant, podem aplicar-ho a la resta:\n",
    "\n",
    "$$x_1 = x_2 / 2 + x_3 / 2 + x_4 / 3$$\n",
    "$$x_2 = x_1 / 2 + x_4 / 3$$\n",
    "$$x_3 = x_4 / 3$$\n",
    "$$x_4 = x_1 / 2 + x_2 / 2 + x_3 / 2$$\n",
    "\n",
    "Ara podríem resoldre aquest sistema per trobar quina és la importància de cada web, un vector $(s_1,s_2,s_3,s_4)$. Però, resoldre-ho no és trivial (no en casos on tenim millors de webs, és clar!). Per tal de poder avançar, el que se sol fer és resoldre-ho en forma matricial:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "& \\text{Out-links} &\\\\\n",
    "G = &\\begin{bmatrix}\n",
    "    0 & 1/2 & 1/2 & 1/3 \\\\\n",
    "    1/2 & 0 & 0 & 1/3 \\\\\n",
    "    0 & 0 & 0 & 1/3 \\\\\n",
    "    1/2 & 1/2 & 1/2 & 0\n",
    "\\end{bmatrix} & \\text{In-links}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "El sistema ara es converteix en una equació, arquetip fàcilment reconegut pels matemàtics:\n",
    "\n",
    "$$x = Gx$$\n",
    "\n",
    "Trobareu els detalls a la publicació si esteu interesats, realment el que estem intentant és trobar el vector propi que té per valor propi 1. És a dir, resoldre $\\lambda x = Gx$ tal que $\\lambda = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trobant el vector propi\n",
    "\n",
    "Un dels mètodes més eficients, tot i que no el que més, és el mètode de la potència. Aquest, permet trobar el vector propi que té el valor propi més alt (coses de les matemàtiques, es pot demostrar que $\\lambda=1$ serà el més alt). El mètode diu així:\n",
    "\n",
    "$\n",
    "i = 0\\\\\n",
    "\\text{do}\\\\\n",
    "\\hspace{2cm}y_{i+1} = Gx_i\\\\\n",
    "\\hspace{2cm}x_{i+1} = y_{i+1} / ||y_{i+1}||_2\\\\\n",
    "\\hspace{2cm}i = i + 1\\\\\n",
    "\\text{until }||x_{i+1} - x_{i}|| < 10^{-6}\n",
    "$\n",
    "\n",
    "On $x_0$ és un vector normalitzat amb suma 1, per exemple si tenim $n$ webs en total $x_0 = \\textbf{1} / n$, on $\\textbf{1}$ és un vector d'1s de tamany $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programa l'algorisme del mètode de la potència amb numpy, seguint el pseudocodi de dalt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def power_method(G):\n",
    "    \"\"\"\n",
    "    Donada una matriu d'adjecències, en calcula el PageRank.\n",
    "    Mitjançant el mètode de la potència troba el vector propi\n",
    "    de valor propi màxim (1)\n",
    "    \n",
    "    :param G: Matriu a calcular el PageRank\n",
    "    :return: Vector d'importàncies del PageRank (vector\n",
    "        propi amb valor propi més alt)\n",
    "    \"\"\"\n",
    "    #x1 = x = an array of G rows x 1 column normalized \n",
    "    x1 = np.ones((G.shape[0],1))/len(G)\n",
    "    i = 0\n",
    "    norma = 1\n",
    "    while norma > 0.000001:\n",
    "        x = x1\n",
    "        y = np.dot(G,x)\n",
    "        x1 = y/np.linalg.norm(y)\n",
    "        norma = np.linalg.norm(x1 - x)\n",
    "    return x\n",
    "    \n",
    "def solve_eig(G):\n",
    "    \"\"\"\n",
    "    Calcula els vectors i valors propis de la matriu G\n",
    "    mitjançant funcions de numpy. Funció de referència\n",
    "    que us pot servir per comprovar que el vostre mètode\n",
    "    power_method retorna el que toca.\n",
    "    \n",
    "    :param G: Matriu a calcular el PageRank\n",
    "    :return: Vector d'importàncies del PageRank (vector\n",
    "        propi amb valor propi més alt)\n",
    "    \"\"\"\n",
    "    vals, vecs = np.linalg.eig(G)\n",
    "    idxs = np.argsort(np.real(vals))\n",
    "    return np.abs(vecs[:, idxs[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.5   0.5   0.33]\n",
      " [ 0.5   0.    0.    0.33]\n",
      " [ 0.    0.    0.    0.33]\n",
      " [ 0.5   0.5   0.5   0.  ]]\n",
      "Eigenvector [[ 0.56]\n",
      " [ 0.49]\n",
      " [ 0.21]\n",
      " [ 0.63]]\n",
      "Eigenvector [ 0.56  0.49  0.21  0.63]\n",
      "Eigenvalues [ 1.0+0.j -0.0+0.j -0.5+0.j -0.5-0.j]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    G1 = np.asarray((\n",
    "        (0,     1/2.0, 1/2.0, 1/3.0), \n",
    "        (1/2.0, 0,     0,     1/3.0), \n",
    "        (0,     0,     0,     1/3.0), \n",
    "        (1/2.0, 1/2.0, 1/2.0, 0)\n",
    "    ))\n",
    "    x = power_method(G1)\n",
    "    y = solve_eig(G1)\n",
    "    \n",
    "    print(np.round(G1, 2))\n",
    "    print('Eigenvector', np.round(x, 2))\n",
    "    print('Eigenvector', np.round(y, 2))\n",
    "    print('Eigenvalues', np.round(np.linalg.eigvals(G1), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casos extrems\n",
    "\n",
    "Evidentment, no tot és tant bonic com sembla... Crea la matriu de la següent configuració i prova que passa quan n'executes el mètode de la potència:\n",
    "\n",
    "<img src=\"img/page2.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   0.5  0.5  0.   0. ]\n",
      " [ 0.5  0.   0.5  0.   0. ]\n",
      " [ 0.5  0.5  0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   1. ]\n",
      " [ 0.   0.   0.   1.   0. ]]\n",
      "Eigenvector [[ 0.45]\n",
      " [ 0.45]\n",
      " [ 0.45]\n",
      " [ 0.45]\n",
      " [ 0.45]]\n",
      "Eigenvector [ 0.    0.    0.    0.71  0.71]\n",
      "Eigenvalues [-0.5  1.  -0.5  1.  -1. ]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    G2 = np.asarray((\n",
    "        (0,     1/2.0, 1/2.0, 0,   0), \n",
    "        (1/2.0, 0,     1/2.0, 0,   0), \n",
    "        (1/2.0, 1/2.0, 0,     0,   0), \n",
    "        (0,     0,     0,     0,   1.0),\n",
    "        (0,     0,     0,     1.0, 0)\n",
    "    ))\n",
    "    x = power_method(G2)\n",
    "    y = solve_eig(G2)\n",
    "    \n",
    "    print(np.round(G2, 2))\n",
    "    print('Eigenvector', np.round(x, 2))\n",
    "    print('Eigenvector', np.round(y, 2))\n",
    "    print('Eigenvalues', np.round(np.linalg.eigvals(G2), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encara en trobem un més de cas extrem:\n",
    "\n",
    "<img src=\"img/page3.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   0.   0.5  0. ]\n",
      " [ 0.   0.   0.5  0. ]\n",
      " [ 1.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0. ]]\n",
      "Eigenvector [ 0.5   0.5   0.71  0.  ]\n",
      "Eigenvalues [ 0.    0.71 -0.71  0.  ]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    G3 = np.asarray((\n",
    "        (0,   0, 1/2.0, 0), \n",
    "        (0,   0, 1/2.0, 0), \n",
    "        (1.0, 0, 0,     0), \n",
    "        (0,   0, 0,     0)\n",
    "    ))\n",
    "    #x = power_method(G3)\n",
    "    y = solve_eig(G3)\n",
    "    \n",
    "    print(np.round(G3, 2))\n",
    "    print('Eigenvector', np.round(y, 2))\n",
    "    print('Eigenvalues', np.round(np.linalg.eigvals(G3), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solucions\n",
    "\n",
    "Per evitar tenir cicles, graphs separats i dangling nodes, el que es fa és modificar la matriu G amb \"soroll\", per tal de que tot quedi connectat amb tot. Aquesta tècnica a vegades rep el nom de \"Random Surfer\".\n",
    "\n",
    "$$M = (1 - m)G + mS$$\n",
    "\n",
    "On $S$ és una matriu amb totes les entrades $1/n$ i $m$ un nombre petit, normalment $0.15$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fes una funció que donada la matriu $G$ i $m$ calculi la nova matriu $M$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_matrix(G, m=0.15):\n",
    "    S = np.ones((G.shape[0],G.shape[1]))/G.shape[1]\n",
    "    return np.dot((1-m),G) + np.dot(m,S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03  0.45  0.45  0.03  0.03]\n",
      " [ 0.45  0.03  0.45  0.03  0.03]\n",
      " [ 0.45  0.45  0.03  0.03  0.03]\n",
      " [ 0.03  0.03  0.03  0.03  0.88]\n",
      " [ 0.03  0.03  0.03  0.88  0.03]]\n",
      "Eigenvector [[ 0.45]\n",
      " [ 0.45]\n",
      " [ 0.45]\n",
      " [ 0.45]\n",
      " [ 0.45]]\n",
      "Eigenvector [ 0.    0.    0.    0.71  0.71]\n",
      "Eigenvector [ 0.45  0.45  0.45  0.45  0.45]\n",
      "Eigenvalues [-0.43  1.    0.85 -0.42 -0.85]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    M2 = fix_matrix(G2)\n",
    "    x = power_method(M2)\n",
    "    y = solve_eig(G2)\n",
    "    z = solve_eig(M2)\n",
    "    \n",
    "    print(np.round(M2, 2))\n",
    "    print('Eigenvector', np.round(x, 2))\n",
    "    print('Eigenvector', np.round(y, 2))\n",
    "    print('Eigenvector', np.round(z, 2))\n",
    "    print('Eigenvalues', np.round(np.linalg.eigvals(M2), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04  0.04  0.46  0.04]\n",
      " [ 0.04  0.04  0.46  0.04]\n",
      " [ 0.89  0.04  0.04  0.04]\n",
      " [ 0.04  0.04  0.04  0.04]]\n",
      "Eigenvector [[ 0.51]\n",
      " [ 0.51]\n",
      " [ 0.69]\n",
      " [ 0.09]]\n",
      "Eigenvector [ 0.5   0.5   0.71  0.  ]\n",
      "Eigenvector [ 0.51  0.51  0.69  0.09]\n",
      "Eigenvalues [ 0.72 -0.6   0.03  0.  ]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    M3 = fix_matrix(G3)\n",
    "    x = power_method(M3)\n",
    "    y = solve_eig(G3)\n",
    "    z = solve_eig(M3)\n",
    "    \n",
    "    print(np.round(M3, 2))\n",
    "    print('Eigenvector', np.round(x, 2))\n",
    "    print('Eigenvector', np.round(y, 2))\n",
    "    print('Eigenvector', np.round(z, 2))\n",
    "    print('Eigenvalues', np.round(np.linalg.eigvals(M3), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomanant\n",
    "\n",
    "El primer que haurem de fer és construir una matriu que ens serveixi, d'alguna forma, com a indicatiu de preferències de cada persona. Per tal efecte, construirem una matriu $m\\times n$, de $m$ usuaris per $n$ items, on cada entrada $i,j$ serà el nombre de vegades que la persona $i$ a comprat l'item $j$.\n",
    "\n",
    "<img src=\"img/Mat.png\">\n",
    "\n",
    "Per saber de quin usuari és cada `order_id`, haureu de creaur el dataset `order_products` amb el `orders`. Una sola persona/usuari tindrà més d'una ordre, mireu quants cops ha comprat els mateixos productes.\n",
    "\n",
    "A més, les dades es composen de molts `product_id` diferents, hi ha massa diversitat entre usuaris. Per tant, per poder recomanar el que farem serà agregar les dades, enlloc de treballar per `product_id` ho farem per `aisle_id`, és a dir \"la secció\" del súper on es troba.\n",
    "\n",
    "Al llarg de la pràctica es parlarà de producte i/o item, doncs és la terminologia estàndard de recomanadors, però sempre serà en referència a `aisle_id` per aquesta pràctica!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from os.path import join, dirname\n",
    "\n",
    "def locate(*path):\n",
    "    base = globals().get('__file__', '.')\n",
    "    return join(dirname(base), *path)\n",
    "\n",
    "def unzip(file):\n",
    "    zip_ref = zipfile.ZipFile(locate(file), 'r')\n",
    "    zip_ref.extractall(locate('data'))\n",
    "    zip_ref.close()\n",
    "\n",
    "unzip('order_products__train.csv.zip')\n",
    "unzip('orders.csv.zip')\n",
    "unzip('products.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df_order_prods = pd.read_csv(locate('data', 'order_products__train.csv'))\n",
    "    df_orders = pd.read_csv(locate('data', 'orders.csv'))[['order_id', 'user_id']]\n",
    "    df_prods = pd.read_csv(locate('data', 'products.csv'))[['product_id', 'aisle_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ### Creua df_order_prods i df_orders\n",
    "    df_merged = pd.merge(df_orders,df_order_prods)    \n",
    "    ### Creua l'anterior amb df_products\n",
    "    df_merged = pd.merge(df_prods, df_merged)\n",
    "    df_merged = df_merged[['product_id','aisle_id','user_id']]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fes la funció que retorna els productes comprats en cada `aisle_id` per cada `user_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_counts_table(df):\n",
    "    \"\"\"\n",
    "    Retorna un dataframe on les columnes són els `aisle_id`, les files `user_id` i els valors\n",
    "    el nombre de vegades que un usuari ha comprat un producte d'un `aisle_id`\n",
    "    \n",
    "    :param df: DataFrame original després de creuar-lo\n",
    "    :return: DataFrame descrit adalt\n",
    "    \"\"\"\n",
    "    return df.groupby([\"user_id\", \"aisle_id\"]).size().unstack().fillna(0)\n",
    "\n",
    "def get_count(df, user_id, aisle_id):\n",
    "    \"\"\"\n",
    "    Retorna el nombre de vegades que un usuari ha comprat en un `aisle_id`\n",
    "    \n",
    "    :param df: DataFrame retornat per `build_counts_table`\n",
    "    :param user_id: ID de l'usuari\n",
    "    :param aisle_id: ID de la secció\n",
    "    :return: Enter amb el nombre de vegades que ha comprat\n",
    "    \"\"\"\n",
    "    return df.loc[user_id][aisle_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    df_counts = build_counts_table(df_merged)\n",
    "    count = get_count(df_counts, 1, 16)\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenim moltes dades en el nostre dataset, pel que és convenient que les reduïm una mica. Per començar a treballar recomanem que reduiu el tamany a aproximadament 0.001 de l'original (`frac=0.001`). Podeu provar, més endavant, amb 0.01.\n",
    "\n",
    "A més, necessitem poder provar quan bé funciona el nostre sistema. Pel que dividirem les dades de cada usuari en 2 parts:\n",
    "\n",
    "1. **Train**: Els items que farem servir per entrenar el nostre recomanador\n",
    "2. **Test**: Dades \"ocultes\" que ens serviran per provar quant bé funciona el sistema\n",
    "\n",
    "**Nota** Pot tardar bastant aquesta cel·la!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_test(df):\n",
    "    \"\"\"\n",
    "    No modifica l'estructura del DataFrame original,\n",
    "    únicament el divideix en 2 sub-DataFrame's.\n",
    "    \n",
    "    Tots dos tenen el mateix nombre d'usuaris, però cada\n",
    "    un té un conjunt diferent de producte d'aquest\n",
    "    \n",
    "    :param df: DataFrame retornat per `build_counts_table`\n",
    "    :return: Dos DataFrames amb diferents productes\n",
    "    \"\"\"\n",
    "    split = lambda i: (\n",
    "        train_test_split(row[row > 0], test_size=0.3, random_state=uid)[i] \\\n",
    "        for uid, row in df.iterrows()\n",
    "    )\n",
    "    train = pd.DataFrame(split(0)).fillna(0)\n",
    "    test = pd.DataFrame(split(1)).fillna(0)\n",
    "    return train, test\n",
    "\n",
    "if __name__ == '__main__':    \n",
    "    df_counts_train, df_counts_test = split_train_test(df_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131, 134)\n",
      "(131, 134)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    FRAC = 0.001\n",
    "    df_reduced_counts_train = df_counts_train.sample(frac=FRAC, random_state=1)\n",
    "    df_reduced_counts_test = df_counts_test.sample(frac=FRAC, random_state=1)\n",
    "    \n",
    "    print(df_reduced_counts_test.shape)\n",
    "    print(df_reduced_counts_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph ampliat\n",
    "\n",
    "Si ara construíssim un graph com el que fèiem per les webs i generessim el vector principal, obtindríem efectívament un vector amb la importància de cada persona... però, relativa a que?\n",
    "\n",
    "<img src=\"img/Matvs.png\">\n",
    "\n",
    "Per tal de solucionar aquest problema, on no sabem que és que del pagerank resultant, el que farem serà ampliar el graph, en certa forma duplicant la informació que tenim. \n",
    "\n",
    "<img src=\"img/Matext.png\">\n",
    "\n",
    "Hauràs de construir una matriu $m+n \\times n+m$, on:\n",
    "\n",
    "* Les $m$ primeres files i les $n$ últimes columnes (indexos $0,m$) sigui la matriu que has construit anteriorment **normalitzada**\n",
    "* Les últimes $n$ files i les primeres $m$ columnes (indexos $m,0$) sigui la matriu anterior però transposada i **normalitzada**\n",
    "* La resta d'entrades, 0\n",
    "\n",
    "\n",
    "**normalitzada**: Aquesta matriu $m\\times n$ ha d'estar normalitzada per columnes (les columnes han de sumar 1). Per simplificar les imatges i que siguin més entenedores, es fan servir els valors reals. Però és molt important que normalitzeu en el vostre codi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_graph(df_train):\n",
    "    \"\"\"\n",
    "    Calcula el graf ampliat de prodcutes i usuaris a partir de l'original\n",
    "    \n",
    "    :param df: Sub-DataFrame del retornat per `build_counts_table`, per training\n",
    "    :return: El graf ampliat, tal i com està descrit adalt, tenint en compte\n",
    "        que la suma de cada columna ha de ser 1 (normalitzar les columnes, és a dir\n",
    "        dividir cada número de la columna pel total de la suma de la mateixa columna)\n",
    "    \"\"\"          \n",
    "    #m = num files\n",
    "    #n = num columnes\n",
    "    m, n = df_train.shape\n",
    "    #we create an array of m+n x n+m length of zeros\n",
    "    output = pd.DataFrame(np.zeros((m+n, n+m)))\n",
    "    #we set the m first rows and n last columns with df_train\n",
    "    #the last n rows and m first columns with with df_train transposed  \n",
    "    output.iloc[:m, -n:],output.iloc[-n:, :m]  = df_train.values, df_train.T.values\n",
    "    #Then we normalize the output filling Na with zeros\n",
    "    output = output.divide(output.sum(1), axis='columns').fillna(0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    G = get_extended_graph(df_reduced_counts_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recomanació personalitzada\n",
    "\n",
    "\n",
    "Seguim tenint un altre problema, i és, estem personalitzant res? La matriu ampliada és exactament la mateixa per cada usuari, independentment del que hagi comprat, i per tant el resultat serà sempre el mateix.\n",
    "\n",
    "Suposa que volem recomanar a l'usuari 1, el que farem serà crear una altre matriu del mateix tamany que l'anterior, on tots els elements seran 0 excepte aquelles files i columnes (corresponents a la matriu ampliada anterior) dels items que ha comprat l'usuari.\n",
    "\n",
    "<img src=\"img/Matper.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalment, la matriu sobre la qual farem el càlcul de pagerank serà la matriu ampliada perturbada per aquesta nova matriu.\n",
    "\n",
    "Anomena $G$ a la original i $E$ a aquesta que acabes de fer, i $\\bar{E}$ és $E$ normalitzada per columnes, la matriu final $G_m$ serà:\n",
    "\n",
    "$$G_m = (1-m)G + m\\bar{E}$$\n",
    "\n",
    "Que ja us hauria de sonar! Ho hem fet abans amb pagerank. Fixeu-vos que per cada usuari la matriu $G_m$ serà diferent, doncs tot i que $G$ no canvia, sí que ho fa $E$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personalize(G, df_train, user, m=0.15):\n",
    "    \"\"\"\n",
    "    Personalitza el graf ampliat per a un usuari donat.\n",
    "    \n",
    "    La matriu E, un cop construida i abans de fer-la servir per personalitzar G,\n",
    "    s'ha de normalitzar per columnes.\n",
    "    \n",
    "    :param G: El graf ampliat\n",
    "    :param df: Sub-DataFrame del retornat per `build_counts_table`, per training\n",
    "    :param user: ID d'usuari\n",
    "    :param m: Valor de perturbació, tal i com està descrit adalt\n",
    "    :return: Matriu ampliada personalitzada\n",
    "    \"\"\"\n",
    "    #we init E matrix full of zeros\n",
    "    E = pd.DataFrame(np.zeros(G.shape))\n",
    "    #we extract the m,n row and column of the user in the df_train graph\n",
    "    m = np.where(df_train.index.values == user)[0][0]\n",
    "    n = np.where(df_train.T.columns.values == user)[0][0]\n",
    "    #and we just set to 1 those row column indexes\n",
    "    E.iloc[int(df_train.shape[1])+int(m)] = 1\n",
    "    E.iloc[:,int(df_train.shape[1])+int(n)] = 1\n",
    "    #we normalize it so the sum of those columns is 1\n",
    "    E = E.divide(E.sum(1), axis='columns').fillna(0)\n",
    "    #then we apply Gm = (1-m)G - mE\n",
    "    return G.multiply((1-m)).add(E.multiply(m), fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    Gm = personalize(G, df_reduced_counts_train, 93427)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalment, ara que ja tenim $G_m$, podem executar pagerank i obtenir el vector principal. Com pots observar a la última imatge, aquest vector tindrà $m+n$ elements, els primers $m$ corresponents als usuaris i els següents $n$ als items.\n",
    "\n",
    "Com que volem similituds entre usuaris, ens quedarem solament amb la primera part, fins a $m$. A més, el propi usuari a qui hem personalitzat la matriu no l'hem de tenir en compte, així que cal posar-l'ho a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sims_vect(Gm, df_train, user):\n",
    "    \"\"\"\n",
    "    Calcula el vector de similituds per a un usuari donat, és a dir\n",
    "    executa el metòde de la potència sobre el graf ampliat personalitzat\n",
    "    de l'usuari, i en retornaels primers M elements del vector resultant.\n",
    "    \n",
    "    A més, posa a 0 la posició del vector corresponent a l'usuari al que\n",
    "    estem recomanant.\n",
    "    \n",
    "    :param Gm: Graf ampliat personalitzat\n",
    "    :param df: Sub-DataFrame del retornat per `build_counts_table`, per training\n",
    "    :param user: ID de l'usuari\n",
    "    :return: Vector de similituds en una array de numpy\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Gm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-01883f8885c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0msims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msims_vect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_reduced_counts_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m93427\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Gm' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    sims = sims_vect(Gm, df_reduced_counts_train, 93427)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara aplica la formula per recomanacions colaboratives donat un usuari $u$ i item $i$\n",
    "\n",
    "$$\\hat{r}_{u,i} = \\frac{\\sum_{p,p\\neq u} sim(u,p) \\cdot r_{p,i}}{\\sum_p sim(u,p)}$$\n",
    "\n",
    "Tingues en compte que aquesta fòrmula solament té en compte aquells usuaris que també han comprat el mateix! Si no han comprat, no s'ha de comptar en el sumatori!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(df_train, user, item, sims):\n",
    "    \"\"\"\n",
    "    Fent servir la fòrmula del filtratge colaboratiu, retorna un valor\n",
    "    per a un usuari i producte\n",
    "    \n",
    "    :param df: Sub-DataFrame del retornat per `build_counts_table`, per training\n",
    "    :parma user: ID de l'usuari\n",
    "    :param item: ID de l'item\n",
    "    :param sims: Vector de similituds per a l'usuari\n",
    "    :return: Un flotant indicant el valor computat segons la fòrmula d'adalt\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_reduced_counts_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-4639b4244460>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_reduced_counts_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m93427\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m98\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_reduced_counts_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m93427\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m98\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_reduced_counts_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m93427\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m98\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_reduced_counts_train' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(score(df_reduced_counts_train, 93427, 98, sims))\n",
    "    print(df_reduced_counts_test.loc[93427, 98])\n",
    "    print(df_reduced_counts_train.loc[93427, 98])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara, donat un usuari, recomana-li els $k$ millors productes que podria comprar. Per fer-ho, computa l'`score` per a cada possible item que encara no hagi comprat, ordena i retorna els $k$ millors.\n",
    "\n",
    "Si $k=0$, significa retornar tots els possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend(df_train, sims, user, k, score=score):\n",
    "    \"\"\"\n",
    "    Calcula l'`score` de tots els items d'un usuari, que no hagin estat ja comprats,\n",
    "    i retorna els $k$ amb valor més alt. Si $k=0$, els retorna tots ordenats de major a menor.\n",
    "    \n",
    "    :param df: Sub-DataFrame del retornat per `build_counts_table`, per training\n",
    "    :param sims: Vector de similituds\n",
    "    :param user: ID de l'usuari\n",
    "    :param k: Número de valors a retornar, o 0 per tots\n",
    "    :param score: Funció a fer servir per calcular l'score\n",
    "    :return: Llista amb els $k$ (o tots si $k=0$) items més alts. Cada element d'aquest\n",
    "        vector serà una tupla (valor, aisle_id)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_reduced_counts_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d0b44c56a961>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecommend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_reduced_counts_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m93427\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_reduced_counts_train' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(recommend(df_reduced_counts_train, sims, 93427, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluant\n",
    "\n",
    "Per saber si em fet un bon recomanador, hem d'avaluar si està funcionant correctament. Ho farem predint la puntuació de tots els items de test per un usuari, i comparant amb els valors reals.\n",
    "\n",
    "Les funcions ja estan fetes, simplement podeu executar per veure que us surt un nombre raonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(df_train, df_test, sims, user, score=score):\n",
    "    real = df_test.loc[user]\n",
    "    pred_list = recommend(df_train, sims, user, 0, score=score)\n",
    "    pred = pd.Series({y: x for x, y in pred_list})\n",
    "    \n",
    "    real = real[real > 0]\n",
    "    pred = pred.loc[real.index]\n",
    "    \n",
    "    return np.sum(np.power(real - pred, 2))\n",
    "        \n",
    "\n",
    "def mean_eval(df_train, df_test, users, score=score):\n",
    "    G = get_extended_graph(df_train)\n",
    "    return np.mean([\n",
    "        evaluate(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            sims_vect(personalize(G, df_train, uid), df_train, uid), \n",
    "            uid,\n",
    "            score=score\n",
    "        ) \\\n",
    "        for uid in users if df_train.loc[uid].sum() > 0\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_reduced_counts_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-7624f9c085e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_reduced_counts_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_reduced_counts_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m93427\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_reduced_counts_train' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    evaluate(df_reduced_counts_train, df_reduced_counts_test, sims, 93427)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_reduced_counts_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-957eaecd0bd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0musers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_reduced_counts_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmean_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_reduced_counts_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_reduced_counts_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_reduced_counts_test' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    users = df_reduced_counts_test.sample(n=10).index\n",
    "    mean_eval(df_reduced_counts_train, df_reduced_counts_test, users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propostes de millora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Millorar la recomanació colaborativa\n",
    "\n",
    "La fòrmula que fem servir per calcular l'`score`, basada en la recomanació colaborativa, és força inexacta, doncs no té en compte el *bias* introduit per la mitja del comprador.\n",
    "\n",
    "Per exemple, jo potser tinc família numerosa i compro sempre 5 del mateix producte com a mínim, mentre que algú que visqui sol únicament en compraria 1 unitat.\n",
    "\n",
    "Podem eliminar aquest bias fent:\n",
    "\n",
    "$$\\hat{r}_{u,i} = \\frac{\\sum_{v,v\\neq u} sim(u,v) \\cdot (r_{v,i} - \\mu_v)}{\\sum_{v,v\\neq u} sim(u,v)} + \\mu_u$$\n",
    "\n",
    "És a dir, a cada producte d'altres persones li restem la mitja d'aquella persona ($\\mu_v$) i al final reintroduïm la mitja de l'usuari a qui estem recomanant ($\\mu_u$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_mean(df_train, user, item, sims):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_reduced_counts_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-bbe689840054>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_reduced_counts_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m93427\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m98\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_reduced_counts_train' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(score_mean(df_reduced_counts_train, 93427, 98, sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_reduced_counts_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-cfc58deff6ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmean_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_reduced_counts_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_reduced_counts_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscore_mean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_reduced_counts_train' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    mean_eval(df_reduced_counts_train, df_reduced_counts_test, users, score=score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Recalcular les mitges cada cop és molt lent, probablement voldràs tenir-les precalculades (amb una variable global o semblant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilitzar més dades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una simple inspecció de la taula de comptes, ens mostrarà que un gran percentatge està buida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-c05644dbe2dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mzero\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf_counts\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnonzero\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mzero\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_counts' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    total = np.prod(df_counts.shape)\n",
    "    zero = (df_counts == 0).sum().sum()\n",
    "    nonzero = total - zero\n",
    "\n",
    "    print('Are 0: {:2.3g}%'.format(zero / total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per tant, podríem pensar que enlloc de guardar absolutament tot, solament necessitem saber les posicions on no hi ha 0's i el seu valor. Això és precisament el que fan les estructures de la llibreria\n",
    "\n",
    "```python\n",
    "import scipy.sparse as sparse\n",
    "```\n",
    "\n",
    "Tot el contingut de la llibreria `sparse` són presentacions no denses de matrius, únicament guarden els elements que són diferents de 0.\n",
    "\n",
    "Es proposa que canvieu les funcions `get_extended_matrix` i `personalize` per tal de que facin servir `sparse.lil_matrix` enlloc de `np.array`, i que augmenteu `FRAC` a `1.0` (totes les dades)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: No es tracta d'un procés trivial, treballar amb matrius sparse té cert misteri. \n",
    "\n",
    "**1.** No feu servir mai funcions de numpy sobre una matriu sparse, per exemple, supossa que `mat` és una `sparse.lil_matrix` i `dia` una matriu diagonal també `sparse.lil_matrix`:\n",
    "\n",
    "```python\n",
    "res = np.dot(mat, dia)\n",
    "```\n",
    "\n",
    "Farà el producte matricial, però la matriu resultant `res` no serà sparse sinó densa. Feu servir sempre la versió \"metòdica\" de les funcions:\n",
    "\n",
    "```python\n",
    "res = mat.dot(dia)\n",
    "```\n",
    "\n",
    "I ara, `res` és sparse, tal i com s'espera.\n",
    "\n",
    "------------------\n",
    "\n",
    "**2.** Si vols agafar una columna sencera i convertir-la a un vector dens, tal i com faria numpy, no és suficient fent:\n",
    "\n",
    "```python\n",
    "col = mat[:,0]\n",
    "```\n",
    "\n",
    "Doncs el resultat, no és un vector de $n$ elements, sinó una matriu $n,1$. Podríem pensar que aplanar el vector resultaria:\n",
    "\n",
    "```python\n",
    "col = mat[:,0].flatten() # O, equivalentment, mat[:,0].ravel()\n",
    "``` \n",
    "\n",
    "Però tampoc funcionarà. Cal convertir explícitament a array de numpy abans:\n",
    "\n",
    "```python\n",
    "col = np.array(mat[:,0]).ravel() # o flatten()\n",
    "```\n",
    "\n",
    "I, ara sí, tenim un vector de $n$ elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
